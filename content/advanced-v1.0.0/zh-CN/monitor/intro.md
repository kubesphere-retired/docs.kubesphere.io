---
title: "监控概述"
---

为 KubeSphere 提供准确而全面的监控环境是保证平台所有容器工作负载和服务的高可靠性、高可用性和高性能的重要部分，提取平台各项资源运行的关键指标，以监控图表形式展示。用户可以非常方便地统计和收集到不同资源在不同维度的监控数据，能够在第一时间掌握资源和业务的运行情况和监控状况，快速定位故障。

收集资源的监控数据旨在帮助用户观察和建立资源和集群性能的正常标准。通过不同时间、不同负载条件下监测集群各项基础指标并收集历史监控数据，并以图表的形式展现容器集群和服务运行时的性能。例如，用户可以监控集群和节点的 CPU 利用率、内存使用率和磁盘 I/O、网卡流量等基础指标，还可以监控企业空间、容器组和容器的 CPU 使用量、内存使用量等指标，以及项目资源用量和服务组件的状态。

### 监控指标

平台对资源的监控从两条线提供多维度的监控指标，即
- 管理员视角：**Cluster -> Node -> Pod -> Container** 
- 用户视角：**Cluster -> Workspace -> Namespace -> Workload/Pod -> Container**

![监控指标](/monitor-items.svg)

从上图中不难发现，平台的监控指标和 IaaS 层相似，也就是我们常见的 CPU、内存、磁盘 和 网络等四个方面的使用量和使用率。另外，我们也提供 Inode 监控。比如有可能用户使用过程中发现 Pod 创建不成功，经过调查发现是由于 inode 满了所导致的。为什么会出现这种情况？是因为 Kubernetes 对镜像和日志都有回收机制，但是对 inode 的回收和清理是没有的，所以说 Inode 监控也是相当重要，它可能会造成你整个集群中某个节点无法创建工作负载。值得一提的是，监控页面支持用户按用量排序和自定义时间范围查询，帮助快速定位故障所在。

### 监控设计架构

我们监控模块的设计和架构，主要是采用了开源的解决方案，大家如果关注云原生和容器监控就会了解，容器的监控基本就这么几种解决方案，我们在最初的技术选型历程，考虑了 Heapster + Elasticsearch 和 Heapster + Kafka + ClickHouse，但最后还是选择了 [Prometheus](https://prometheus.io/)。

因为调研的时候，我们发现 Heapster 要被 Metrics Server 取代，只把当前监控数据保存在内存中，需要有工具去抓取，不能再依赖 Heapster 抓取后存到各种后端存储。而选择 Prometheus 的理由不仅仅是因为它和 Kubenetes 天然集成，为 CloudNative 而设计，多种工具和服务纷纷推出适配的 Exporter，供 Prometheus 抓取，接触到的信息是越来越多的人利用 Prometheus + Grafana 监控 K8S 集群，逐渐成为监控告警的首选技术，并且拥有灵活好用的查询语言 PromQL 和能把 PromQL  嵌入 http 请求的强大的 API，以及单节点能处理海量数据的能力。

在提供了监控之后，我们将在下一个版本支持创建告警和消息通知的功能，根据给定阈值每隔一段时间发送告警，比如设置告警策略类型、告警规则和接收方式。下图是我们监控以及即将集成的告警模块的设计架构。

![架构](/monitoring-design.png)


